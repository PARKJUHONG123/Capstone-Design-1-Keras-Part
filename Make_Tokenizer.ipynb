{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "import io\n",
    "import json\n",
    "\n",
    "#mpname mpstand pname pmake\n",
    "with open('pname_over200.csv') as file_pname:\n",
    "    pname_data = []\n",
    "    for line in file_pname.readlines():\n",
    "        pname_data.append(line.split(','))\n",
    "\n",
    "with open('pmake_over200_16m.csv') as file_pmake:\n",
    "    pmake_data = []\n",
    "    for line in file_pmake.readlines():\n",
    "        pmake_data.append(line.split(','))\n",
    "        \n",
    "mpname_pname = []\n",
    "mpstand_pname = []\n",
    "\n",
    "mpname_pmake = []\n",
    "mpstand_pmake = []\n",
    "\n",
    "pname = []\n",
    "pmake = []\n",
    "\n",
    "mpname_pname = np.array(pname_data)[:, 0]\n",
    "mpstand_pname = np.array(pname_data)[:, 1]\n",
    "mpbase_pname = [\"\" for row in range(len(pname_data))]\n",
    "\n",
    "mpname_pmake = np.array(pmake_data)[:, 0]\n",
    "mpstand_pmake = np.array(pmake_data)[:, 1]\n",
    "mpbase_pmake = [\"\" for row in range(len(pmake_data))]\n",
    "\n",
    "pname = np.array(pname_data)[:, 2]\n",
    "pmake = np.array(pmake_data)[:, 3]\n",
    "\n",
    "for i in range(len(pname_data)):\n",
    "    mpbase_pname[i] = mpname_pname[i] + mpstand_pname[i]\n",
    "\n",
    "for i in range(len(pmake_data)):\n",
    "    mpbase_pmake[i] = mpname_pmake[i] + mpstand_pmake[i]\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "mpbase_pname_tokenizer = Tokenizer()\n",
    "mpbase_pmake_tokenizer = Tokenizer()\n",
    "pmake_tokenizer = Tokenizer()\n",
    "pname_tokenizer = Tokenizer()\n",
    "\n",
    "mpbase_pname_tokenizer.fit_on_texts(mpbase_pname)\n",
    "mpbase_pmake_tokenizer.fit_on_texts(mpbase_pmake)\n",
    "pmake_tokenizer.fit_on_texts(pmake)\n",
    "pname_tokenizer.fit_on_texts(pname)\n",
    "\n",
    "mpbase_pname_sequences = mpbase_pname_tokenizer.texts_to_sequences(mpbase_pname)\n",
    "mpbase_pmake_sequences = mpbase_pmake_tokenizer.texts_to_sequences(mpbase_pmake)\n",
    "pmake_sequences = pmake_tokenizer.texts_to_sequences(pmake)\n",
    "pname_sequences = pname_tokenizer.texts_to_sequences(pname)\n",
    "\n",
    "mpbase_pname_tokenizer_json = mpbase_pname_tokenizer.to_json()\n",
    "with io.open('mpbase_pname_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(mpbase_pname_tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "mpbase_pmake_tokenizer_json = mpbase_pmake_tokenizer.to_json()\n",
    "with io.open('mpbase_pmake_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(mpbase_pmake_tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "pname_tokenizer_json = pname_tokenizer.to_json()\n",
    "with io.open('pname_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(pname_tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "pmake_tokenizer_json = pmake_tokenizer.to_json()\n",
    "with io.open('pmake_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(pmake_tokenizer_json, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
